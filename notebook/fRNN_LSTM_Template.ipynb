{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Dau.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MASEF, University Paris-Dauphine 2021:   Bryan Delamour "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Hedging Under Rough Volatility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $1)$ Hedging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8YfQe2JfLSR",
    "outputId": "b5f71c91-a305-4fe5-fbdf-a0740a6a56e4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cmath import * \n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import os\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### European Call\n",
    "\n",
    "Parameters: \n",
    "\n",
    "Strike $K = 100$\n",
    "\n",
    "\n",
    "Spot price $S_0 = 100 $\n",
    "\n",
    "Maturity $ T = 30/365 $ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hedging parameters\n",
    "\n",
    "Daily Hedge\n",
    "\n",
    "Number of hedging instruments $ d=2 $ \n",
    "\n",
    "Instruments = asset price, volatility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "feQZPXnOfLSZ",
    "outputId": "6e248b41-758e-438c-ab79-7eb25412bfb3"
   },
   "outputs": [],
   "source": [
    "Product=\"European Call\"\n",
    "K=100 #strike\n",
    "S_0=100 # spot price\n",
    "T=30/365 #Maturity\n",
    "d=2 # nb of hedging instruments (S,V)\n",
    "steps=30 # Daily Hedging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $2)$ Fully Recurrent Neural Network LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVZg406pfLSc"
   },
   "source": [
    "## Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"fRNN_LSTM.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWSPN3R8fLSd"
   },
   "outputs": [],
   "source": [
    "# Fully recurrent LSTM Neural Network with:\n",
    "\n",
    "    # Batch Normalization\n",
    "    # sigmoid activation\n",
    "    # 2 hidden Layers\n",
    "    \n",
    "hidden_dim=12\n",
    "    \n",
    "class fRNN_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(fRNN_LSTM, self).__init__()\n",
    "\n",
    "        self.lstm_layer1 = nn.LSTM(d, hidden_dim, 1, batch_first=True)#input data is of shape (batch_size, seq_len, features) then you need batch_first=True\n",
    "\n",
    "        self.lstm_layer2 = nn.LSTM(2*d+4*hidden_dim, hidden_dim, 2, batch_first=True)\n",
    "\n",
    "        self.output_layer=nn.Linear(hidden_dim, d)\n",
    "\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "        self.BN1= nn.BatchNorm1d(d)\n",
    "\n",
    "        self.LayerN= nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, S):\n",
    "        \n",
    "        batch_size, steps, _ = S.shape\n",
    "        output=torch.zeros(S.shape)\n",
    "\n",
    "        SV0=S[:,0,:].view((batch_size, 1, 2))\n",
    "        \n",
    "        delta, h_c_n =self.lstm_layer1(SV0)#input layer t0\n",
    "        \n",
    "        delta=self.output_layer(delta) #output layer t0\n",
    "\n",
    "        deltaS=self.BN1(delta[:,:,:1])\n",
    "        deltaV=self.BN1(delta[:,:,1:])\n",
    "        \n",
    "        deltaS =self.sigmoid(deltaS) #sigmoid delta >=0\n",
    "        deltaV =self.sigmoid(deltaV)\n",
    "\n",
    "\n",
    "        output[:,0,:1]=deltaS[:,0,:] # append delta0 list \n",
    "        output[:,0,1:]=deltaV[:,0,:]\n",
    "        \n",
    "        hn1=torch.reshape( h_c_n[0], (batch_size, 1, hidden_dim) )\n",
    "        cn1=torch.reshape( h_c_n[1], (batch_size, 1, hidden_dim) )\n",
    "\n",
    "\n",
    "        hn1=self.LayerN(hn1)\n",
    "        cn1=self.LayerN(cn1)\n",
    "\n",
    "        hn2=torch.zeros((batch_size, 1, hidden_dim))\n",
    "        cn2=torch.zeros((batch_size, 1, hidden_dim))\n",
    "\n",
    "        for i in range(1,steps):\n",
    "            \n",
    "            SVt=S[:,i,:].view((batch_size, 1, 2)) #reshape Sti\n",
    "            \n",
    "            St=self.BN1(SVt[:,:,:1]) #BN St\n",
    "            Vt=self.BN1(SVt[:,:,1:]) #BN Vt\n",
    "\n",
    "            y=torch.cat((St,Vt,deltaS,deltaV,hn1,cn1,hn2,cn2),dim=2)      #input (Sti,Vti deltaS ti-1,deltaV ti-1,h_c_n)\n",
    "\n",
    "            delta, h_c_n=self.lstm_layer2(y) #input layer (Sti,delta ti-1,h_c_n)\n",
    "\n",
    "            delta =self.output_layer(delta) #output delta ti \n",
    "\n",
    "            deltaS=self.BN1(delta[:,:,:1])  #BN delta S delta V\n",
    "            deltaV=self.BN1(delta[:,:,1:])\n",
    "\n",
    "            deltaS=self.sigmoid(deltaS) #sigmoid delta>=0\n",
    "            deltaV=self.sigmoid(deltaV)\n",
    "\n",
    "            output[:,i,:1]=deltaS[:,0,:] # append delta ti list\n",
    "            output[:,i,1:]=deltaV[:,0,:] \n",
    "            \n",
    "\n",
    "            hn1=torch.reshape( h_c_n[0], (batch_size, 2, hidden_dim) )\n",
    "            hn1=hn1[:,1:,:]\n",
    "            hn2=hn1[:,:1,:]\n",
    "\n",
    "            hn1=self.LayerN(hn1)\n",
    "            hn2=self.LayerN(hn2)\n",
    "\n",
    "            cn1=torch.reshape( h_c_n[1], (batch_size, 2, hidden_dim) )\n",
    "            cn1=cn1[:,1:,:]\n",
    "            cn2=cn1[:,:1,:]\n",
    "\n",
    "            cn1=self.LayerN(cn1)\n",
    "            cn2=self.LayerN(cn2)\n",
    "\n",
    "        return output #return delta list\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgMulMpsfLSe",
    "outputId": "1bab5c81-1ae5-4469-d25f-5c62e40b021a"
   },
   "outputs": [],
   "source": [
    "fRnn_LSTM=fRNN_LSTM()\n",
    "print(fRnn_LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data importation For our rBergomi class\n",
    "\n",
    "S1_training_File = \"S1_training_data.pkl\"  \n",
    "p_0_training_File = \"p_0_training_data.pkl\"\n",
    "fRnn_LSTM_model_File = \"fRnn_LSTM_model.pkl\"  \n",
    "\n",
    "V1_training_File= \"V1_training_data.pkl\"\n",
    "\n",
    "with open(S1_training_File, 'rb') as file:  \n",
    "    S1_train = pickle.load(file)\n",
    "    \n",
    "with open(p_0_training_File, 'rb') as file:  \n",
    "    p_0 = pickle.load(file)\n",
    "\n",
    "with open(V1_training_File, 'rb') as file:  \n",
    "    V1_train = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $3)$ Network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batching and stochastic gradient descent\n",
    "\n",
    "The sample is divided into mini-batch of size 256 to allow faster computation.\n",
    "\n",
    "The insight of stochastic gradient descent is that the gradient is an expectation. The expectation may be approximately estimated using a small set of samples. Specifically, on each step of the algorithm, we can sample a minibatch of examples $\\mathbb{B}=\\left\\{\\boldsymbol{(S,V)}^{(1)}, \\ldots, \\boldsymbol{(S,V)}^{\\left(m^{\\prime}\\right)}\\right\\}$ drawn uniformly from the training set. \n",
    "\n",
    "\n",
    "The estimate of the gradient is formed as\n",
    "$$\n",
    "g=\\frac{1}{m^{\\prime}} \\nabla_{\\theta} \\sum_{i=1}^{m^{\\prime}} L\\left((S,V)^{(i)}, Z^{(i)}, \\theta\\right)\n",
    "$$\n",
    "using examples from the minibatch $\\mathbb{B}$. \n",
    "\n",
    "The stochastic gradient descent algorithm then follows the estimated gradient downhill:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta-\\epsilon g\n",
    "$$\n",
    "where $\\epsilon$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Deep Learning, Ian Goodfellow Yoshua Bengio Aaron Courville, Chapter 5, p152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is defined as: \n",
    "\n",
    "$$L(S,V,Z,{\\theta})= \\mathbb{E}\\left[\\left(-Z+p_{0}+\\left(\\delta_S^{\\theta} \\cdot S\\right)_{T}+\\left(\\delta_V^{\\theta} \\cdot V\\right)_{T}\\right)^{2}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Mini-Batching the loss is therefore:\n",
    "\n",
    "$$\n",
    "L=\\frac{1}{m}  \\sum_{i=1}^{m} L\\left((S,V)^{(i)}, Z^{(i)}, \\theta\\right)\n",
    "$$\n",
    "\n",
    "Where m is the number of mini-batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(sampleS,sampleV, batch_size):\n",
    "    rnd_idx = np.random.permutation(sampleS.shape[0])\n",
    "    sample_batchS=[]\n",
    "    sample_batchV=[]\n",
    "    n_batches = sampleS.shape[0] // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "      sample_batchS.append(sampleS[batch_idx,:,:])\n",
    "      sample_batchV.append(sampleV[batch_idx,:,:])\n",
    "    return ( (sample_batchS,sample_batchV) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "S1_train500k=S1_train[:499968,:,:]\n",
    "p_0_500k=p_0[:499968,:,:]\n",
    "V1_train500k=V1_train[:499968,:,:]\n",
    "\n",
    "test_S1=S1_train[500000:501000,:,:]\n",
    "test_p_0=p_0[500000:501000,:,:]\n",
    "test_V1=V1_train[500000:501000,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setting \n",
    "\n",
    "learning_rate=0.005\n",
    "batch_size=256\n",
    "epochs=200\n",
    "nb_batch=S1_train500k.shape[0]//batch_size #nb of patch / set\n",
    "optimizer = torch.optim.Adam(fRnn_LSTM.parameters(), lr=learning_rate)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss=10\n",
    "LOSS_B=[]\n",
    "LOSS_test=[]\n",
    "strat=[]\n",
    "pnl_evol=[]\n",
    "mediane=[]\n",
    "rate_timer=0\n",
    "training_rates=[learning_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "\n",
    "t1=time()\n",
    "\n",
    "shuffleS, shuffleV = shuffle_batch(S1_train500k,V1_train500k,batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss_b=0\n",
    "\n",
    "\n",
    "    for sample_batchS, sample_batchV in zip(shuffleS, shuffleV):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input=torch.cat((sample_batchS, sample_batchV),dim=2)\n",
    "\n",
    "        S=sample_batchS[:,1:,:]-sample_batchS[:,:-1,:]\n",
    "        V=sample_batchV[:,1:,:]-sample_batchV[:,:-1,:]\n",
    "        \n",
    "        output = fRnn_LSTM(input)\n",
    "\n",
    "        loss_batch= loss(torch.sum(S*output[:,:-1,:1],dim=1)+torch.sum(V*output[:,:-1,1:],dim=1)+p_0[:batch_size,0,:],torch.max(sample_batchS[:,-1,:]-K,torch.zeros(batch_size, 1)) )\n",
    "         \n",
    "        loss_b += loss_batch/nb_batch    \n",
    "        \n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        min_loss=min(min_loss,loss_batch)\n",
    "      \n",
    "    LOSS_B.append(loss_b.item())\n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Learning Rate\", training_rates[-1] )\n",
    "    print(colored(\"Epoch:\",'red'), epoch,\"loss:\", colored(loss_b.item(),'red') )\n",
    "    print( \"Best Loss\", min(LOSS_B))\n",
    "    print('Best Batch Loss',min_loss , '\\n')\n",
    "    \n",
    "    S=test_S1[:,1:,:]-test_S1[:,:-1,:]\n",
    "    V=test_V1[:,1:,:]-test_V1[:,:-1,:]\n",
    "\n",
    "    input=torch.cat((test_S1,test_V1),dim=2)\n",
    "    output=fRnn_LSTM(input)\n",
    "    x=torch.sum(S*output[:,:-1,:1],dim=1)+torch.sum(V*output[:,:-1,1:],dim=1)-torch.max(test_S1[:,-1,:]-K,torch.zeros(test_S1.shape[0], 1))+test_p_0[:,0,:]\n",
    "\n",
    "    test=loss(torch.sum(S*output[:,:-1,:1],dim=1)+torch.sum(V*output[:,:-1,1:],dim=1)+test_p_0[:,0,:],torch.max(test_S1[:,-1,:]-K,torch.zeros(test_S1.shape[0], 1)))\n",
    "    LOSS_test.append(test)\n",
    "    \n",
    "    if (test<=min(LOSS_test)):\n",
    "        with open(fRnn_LSTM_model_File, 'wb') as file:  \n",
    "            pickle.dump(fRnn, file)      \n",
    "    \n",
    "    strat.append(torch.mean(torch.sum(S*output[:,:-1,:1],dim=1)+torch.sum(V*output[:,:-1,1:],dim=1)).item())\n",
    "    pnl_evol.append(torch.mean( x ).item())\n",
    "    mediane.append(torch.median( x ).item())\n",
    "    quadra=(\"Quadratic loss \", test.item())\n",
    "    print(colored(\"TEST\",'green') )\n",
    "    print(\"Min Test Loss\", min(LOSS_test))\n",
    "    print(\"PnL moyen\", torch.mean( x ).item())\n",
    "    print(colored(quadra,'green'))\n",
    "    print( \"PnL median\", torch.median( x ).item())\n",
    "    print( \"Strat moyenne\", torch.mean(torch.sum(S*output[:,:-1,:1],dim=1)+torch.sum(V*output[:,:-1,1:],dim=1)).item())\n",
    "    print( \"Payoff - p_0 moyen\", torch.mean(-torch.max(test_S1[:,-1,:]-K,torch.zeros(test_S1.shape[0], 1))+test_p_0[:,0,:]).item(), '\\n')\n",
    "\n",
    "\n",
    "t2=time()\n",
    "\n",
    "print( colored('Finale Validation Loss','red') , colored(loss_b.item(),'red'))\n",
    "print( colored('Best Validation Loss', 'green'), min(LOSS_B))\n",
    "print(colored('Finale Test Loss', 'red'), LOSS_test[-1])\n",
    "print(colored('Best Test Loss', 'green' ), min(LOSS_test) )\n",
    "print('Data Size', S1_train500k.shape)\n",
    "print(\"Training Time (h)\", (t2-t1)/3600 ,'\\n') \n",
    "\n",
    "print( \"Evolution during training\")\n",
    "\n",
    "\n",
    "print( \"Quadratic loss evolution\")\n",
    "plt.plot(LOSS_test,color='green')\n",
    "plt.plot(LOSS_B,color='red')\n",
    "plt.show()\n",
    "\n",
    "print(\"Learning Rates\")\n",
    "plt.plot(training_rates)\n",
    "plt.show()\n",
    "\n",
    "print(\"strat test\")\n",
    "plt.plot(strat)\n",
    "plt.show()\n",
    "print( \"PNL Test\")\n",
    "plt.plot(pnl_evol)\n",
    "plt.show()\n",
    "\n",
    "print(\"mediane Test\")\n",
    "plt.plot(mediane)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(x.detach().numpy(),bins=50)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ColabLSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
