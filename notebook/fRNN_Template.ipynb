{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Dau.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MASEF, University Paris-Dauphine 2021:  Bryan Delamour "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Hedging Under Rough Volatility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $1)$ Hedging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dng2YTVXpX_n"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cmath import * \n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import os\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### European Call\n",
    "\n",
    "Parameters: \n",
    "\n",
    "Strike $K = 100$\n",
    "\n",
    "\n",
    "Spot price $S_0 = 100 $\n",
    "\n",
    "Maturity $ T = 30/365 $ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hedging parameters\n",
    "\n",
    "Daily Hedge\n",
    "\n",
    "Number of hedging instruments $ d=2 $ \n",
    "\n",
    "Instruments = asset price, volatility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C7lmLX7Qpg-B"
   },
   "outputs": [],
   "source": [
    "Product=\"European Call\"\n",
    "K=100 #strike\n",
    "S_0=100 # spot price\n",
    "T=30/365 #Maturity\n",
    "d=2 # nb of hedging instruments (S,V)\n",
    "steps=30 # Daily Hedging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $2)$ Fully Recurrent Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Deep Hedging under Rough Volatility, Blanka Horvath, Josef Teichmann, Zan Zuric (2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/2102.01962"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network is fully recurrent in the sense that hidden states are passed as inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"fRNN.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a sequence of Neural Network, each network corresponding to a time $t$.\n",
    "\n",
    "Outputs ($ {\\delta}_t^{S}$, ${\\delta}_t^{V}$) and  hidden states ($ \\tilde{\\delta}_t^{S}$,$\\tilde{\\delta}_t^{V}$ ) of the network at time $t$ are passed at time $t+1$ as inputs.\n",
    "\n",
    "Before each pass, inputs are normalized.\n",
    "\n",
    "The activation function is ReLu.\n",
    "\n",
    "The weights are initialized using Xavier normal and uniform distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b494VPJKpjNC"
   },
   "outputs": [],
   "source": [
    "# Fully recurrent Neural Network with:\n",
    "\n",
    "    # Batch Normalization\n",
    "    # Relu activation\n",
    "    # 2 hidden Layers\n",
    "\n",
    "hidden_dim=12\n",
    "\n",
    "class fRNN(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "      super(fRNN, self).__init__()\n",
    "        \n",
    "      self.rnn_layer1 = nn.RNN(d, hidden_dim, 1, batch_first=True)#input data is of shape (batch_size, seq_len, features) then you need batch_first=True\n",
    "        \n",
    "      self.rnn_layer2 = nn.RNN(2*d+2*hidden_dim, hidden_dim, 2, batch_first=True)\n",
    "        \n",
    "      self.output_layer=nn.Linear(hidden_dim, d)\n",
    "        \n",
    "      self.relu=nn.ReLU()\n",
    "        \n",
    "      self.BN1= nn.BatchNorm1d(1)\n",
    "\n",
    "      self.LayerN= nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "# INITIALIZATION OF WEIGHTS \n",
    "\n",
    "      for name, param in self.rnn_layer1.named_parameters():\n",
    "        if 'weight' in name:\n",
    "          nn.init.xavier_normal_(param)\n",
    "      for name, param in self.rnn_layer2.named_parameters():\n",
    "        if 'weight' in name:\n",
    "          nn.init.xavier_normal_(param)  \n",
    "      for name, param in self.output_layer.named_parameters():\n",
    "        if 'weight' in name:\n",
    "          nn.init.xavier_uniform_(param)    \n",
    "\n",
    "# Feedforward \n",
    "\n",
    "  def forward(self, S):\n",
    "\n",
    "      batch_size, steps, _ = S.shape\n",
    "      output=torch.zeros(S.shape)\n",
    "      \n",
    "      SV0=S[:,0,:].view((batch_size, 1, 2))\n",
    "\n",
    "      delta, h_c_n =self.rnn_layer1(SV0)#input layer t0\n",
    "\n",
    "      delta=self.output_layer(delta) #output layer t0  \n",
    "    \n",
    "      deltaS=self.BN1(delta[:,:,:1])\n",
    "      deltaV=self.BN1(delta[:,:,1:]) ## Batch Normalisation (BN) \n",
    "\n",
    "      deltaS =self.relu(deltaS) #delta >=0\n",
    "      deltaV =self.relu(deltaV)\n",
    "\n",
    "      output[:,0,:1]=deltaS[:,0,:] # append delta0 list \n",
    "      output[:,0,1:]=deltaV[:,0,:]\n",
    "\n",
    "      hn1=torch.reshape( h_c_n, (batch_size, 1, hidden_dim) )\n",
    "\n",
    "      hn1=self.LayerN(hn1)\n",
    "      hn2=torch.zeros((batch_size, 1, hidden_dim))\n",
    "\n",
    "      for i in range(1,steps):\n",
    "            \n",
    "        SVt=S[:,i,:].view((batch_size, 1, 2)) #reshape Sti\n",
    "\n",
    "        St=self.BN1(SVt[:,:,:1]) #BN St\n",
    "        Vt=self.BN1(SVt[:,:,1:]) #BN Vt\n",
    "           \n",
    "        y=torch.cat((St,Vt,deltaS,deltaV,hn1,hn2),dim=2)     #input (Sti, delta ti-1,h_c_n)\n",
    "            \n",
    "        delta, h_c_n=self.rnn_layer2(y) #input layer (Sti,delta ti-1,h_c_n)\n",
    "            \n",
    "        delta =self.output_layer(delta) #output delta ti \n",
    "\n",
    "        deltaS=self.BN1(delta[:,:,:1])  #BN delta S delta V\n",
    "        deltaV=self.BN1(delta[:,:,1:])\n",
    "\n",
    "        deltaS=self.relu(deltaS) #delta>=0\n",
    "        deltaV=self.relu(deltaV)\n",
    "\n",
    "        output[:,i,:1]=deltaS[:,0,:] # append delta ti list\n",
    "        output[:,i,1:]=deltaV[:,0,:]   \n",
    "\n",
    "        hn1=torch.reshape( h_c_n, (batch_size, 2, hidden_dim) )\n",
    "        hn1=hn1[:,1:,:]\n",
    "        hn2=hn1[:,:1,:]\n",
    "        \n",
    "        hn1=self.LayerN(hn1)\n",
    "        hn2=self.LayerN(hn2)\n",
    "\n",
    "      return output #return delta list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cy5_J2Hkpn2n",
    "outputId": "d806d9bf-b7d6-4461-d5be-5c0ef4602515"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fRNN(\n",
      "  (rnn_layer1): RNN(2, 12, batch_first=True)\n",
      "  (rnn_layer2): RNN(28, 12, num_layers=2, batch_first=True)\n",
      "  (output_layer): Linear(in_features=12, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (BN1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (LayerN): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model summary\n",
    "fRnn=fRNN()\n",
    "print(fRnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6LQ3-1Gspqs0"
   },
   "outputs": [],
   "source": [
    "# Data importation For our rBergomi class\n",
    "\n",
    "S1_training_File = \"S1_training_data.pkl\"  \n",
    "p_0_training_File = \"p_0_training_data.pkl\"\n",
    "fRnn_model_File = \"fRnn_model.pkl\"  \n",
    "\n",
    "V1_training_File= \"V1_training_data.pkl\"\n",
    "\n",
    "with open(S1_training_File, 'rb') as file:  \n",
    "    S1_train = pickle.load(file)\n",
    "    \n",
    "with open(p_0_training_File, 'rb') as file:  \n",
    "    p_0 = pickle.load(file)\n",
    "\n",
    "with open(V1_training_File, 'rb') as file:  \n",
    "    V1_train = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $3)$ Network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batching and stochastic gradient descent\n",
    "\n",
    "The sample is divided into mini-batch of size 256 to allow faster computation.\n",
    "\n",
    "The insight of stochastic gradient descent is that the gradient is an expectation. The expectation may be approximately estimated using a small set of samples. Specifically, on each step of the algorithm, we can sample a minibatch of examples $\\mathbb{B}=\\left\\{\\boldsymbol{(S,V)}^{(1)}, \\ldots, \\boldsymbol{(S,V)}^{\\left(m^{\\prime}\\right)}\\right\\}$ drawn uniformly from the training set. \n",
    "\n",
    "\n",
    "The estimate of the gradient is formed as\n",
    "$$\n",
    "g=\\frac{1}{m^{\\prime}} \\nabla_{\\theta} \\sum_{i=1}^{m^{\\prime}} L\\left((S,V)^{(i)}, Z^{(i)}, \\theta\\right)\n",
    "$$\n",
    "using examples from the minibatch $\\mathbb{B}$. \n",
    "\n",
    "The stochastic gradient descent algorithm then follows the estimated gradient downhill:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta-\\epsilon g\n",
    "$$\n",
    "where $\\epsilon$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Deep Learning, Ian Goodfellow Yoshua Bengio Aaron Courville, Chapter 5, p152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is defined as: \n",
    "\n",
    "$$L(S,V,Z,{\\theta})= \\mathbb{E}\\left[\\left(-Z+p_{0}+\\left(\\delta_S^{\\theta} \\cdot S\\right)_{T}+\\left(\\delta_V^{\\theta} \\cdot V\\right)_{T}\\right)^{2}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Mini-Batching the loss is therefore:\n",
    "\n",
    "$$\n",
    "L=\\frac{1}{m}  \\sum_{i=1}^{m} L\\left((S,V)^{(i)}, Z^{(i)}, \\theta\\right)\n",
    "$$\n",
    "\n",
    "Where m is the number of mini-batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RO9JVgeHptKy"
   },
   "outputs": [],
   "source": [
    "def shuffle_batch(sampleS,sampleV, batch_size):\n",
    "    rnd_idx = np.random.permutation(sampleS.shape[0])\n",
    "    sample_batchS=[]\n",
    "    sample_batchV=[]\n",
    "    n_batches = sampleS.shape[0] // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "      sample_batchS.append(sampleS[batch_idx,:,:])\n",
    "      sample_batchV.append(sampleV[batch_idx,:,:])\n",
    "    return ( (sample_batchS,sample_batchV) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BY0aAYSPpvaF"
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "S1_train500k=S1_train[:499968,:,:]\n",
    "p_0_500k=p_0[:499968,:,:]\n",
    "V1_train500k=V1_train[:499968,:,:]\n",
    "\n",
    "test_S1=S1_train[500000:501000,:,:]\n",
    "test_p_0=p_0[500000:501000,:,:]\n",
    "test_V1=V1_train[500000:501000,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXq8dQTtp1MQ",
    "outputId": "b138d1a8-5235-4ed2-c7f7-a2e66e22ac92"
   },
   "outputs": [],
   "source": [
    "# Training setting \n",
    "\n",
    "learning_rate=0.005\n",
    "batch_size=256\n",
    "epochs=200\n",
    "nb_batch=S1_train500k.shape[0]//batch_size #nb of patch / set\n",
    "optimizer = torch.optim.Adam(fRnn.parameters(), lr=learning_rate)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GJk3K75pp3ik"
   },
   "outputs": [],
   "source": [
    "min_loss=10\n",
    "LOSS_B=[]\n",
    "LOSS_test=[]\n",
    "strat=[]\n",
    "pnl_evol=[]\n",
    "mediane=[]\n",
    "rate_timer=0\n",
    "training_rates=[learning_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8AhsYKN0p7Bz",
    "outputId": "711498f0-370a-4de3-9487-1d7f24b61fc4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Training\n",
    "\n",
    "t1=time()\n",
    "\n",
    "shuffleS, shuffleV = shuffle_batch(S1_train500k,V1_train500k,batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss_b=0\n",
    "\n",
    "\n",
    "    for sample_batchS, sample_batchV in zip(shuffleS, shuffleV):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input=torch.cat((sample_batchS, sample_batchV),dim=2)\n",
    "\n",
    "        S=sample_batchS[:,1:,:]-sample_batchS[:,:-1,:]\n",
    "        V=sample_batchV[:,1:,:]-sample_batchV[:,:-1,:]\n",
    "        \n",
    "        output = fRnn(input)\n",
    "\n",
    "        loss_batch= loss(torch.sum(S*output[:,:-1,:1],dim=1)+torch.sum(V*output[:,:-1,1:],dim=1)+p_0[:batch_size,0,:],torch.max(sample_batchS[:,-1,:]-K,torch.zeros(batch_size, 1)) )\n",
    "         \n",
    "        loss_b += loss_batch/nb_batch    \n",
    "        \n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        min_loss=min(min_loss,loss_batch)\n",
    "      \n",
    "    LOSS_B.append(loss_b.item())\n",
    "  \n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Learning Rate\", training_rates[-1] )\n",
    "    print(colored(\"Epoch:\",'red'), epoch,\"loss:\", colored(loss_b.item(),'red') )\n",
    "    print( \"Best Loss\", min(LOSS_B))\n",
    "    print('Best Batch Loss',min_loss , '\\n')\n",
    "    \n",
    "    S=test_S1[:,1:,:]-test_S1[:,:-1,:]\n",
    "    V=test_V1[:,1:,:]-test_V1[:,:-1,:]\n",
    "\n",
    "    input=torch.cat((test_S1,test_V1),dim=2)\n",
    "    output=fRnn(input)\n",
    "    x=torch.sum(S*output[:,:-1,:1],dim=1)+torch.sum(V*output[:,:-1,1:],dim=1)-torch.max(test_S1[:,-1,:]-K,torch.zeros(test_S1.shape[0], 1))+test_p_0[:,0,:]\n",
    "\n",
    "    test=loss(torch.sum(S*output[:,:-1,:1],dim=1)+torch.sum(V*output[:,:-1,1:],dim=1)+test_p_0[:,0,:],torch.max(test_S1[:,-1,:]-K,torch.zeros(test_S1.shape[0], 1)))\n",
    "    LOSS_test.append(test)\n",
    "    \n",
    "    if (test<=min(LOSS_test)):\n",
    "        with open(fRnn_model_File, 'wb') as file:  \n",
    "            pickle.dump(fRnn, file)      \n",
    "    \n",
    "    strat.append(torch.mean(torch.sum(S*output[:,:-1,:1],dim=1)+torch.sum(V*output[:,:-1,1:],dim=1)).item())\n",
    "    pnl_evol.append(torch.mean( x ).item())\n",
    "    mediane.append(torch.median( x ).item())\n",
    "    quadra=(\"Quadratic loss \", test.item())\n",
    "    print(colored(\"TEST\",'green') )\n",
    "    print(\"Min Test Loss\", min(LOSS_test))\n",
    "    print(\"PnL moyen\", torch.mean( x ).item())\n",
    "    print(colored(quadra,'green'))\n",
    "    print( \"PnL median\", torch.median( x ).item())\n",
    "    print( \"Strat moyenne\", torch.mean(torch.sum(S*output[:,:-1,:1],dim=1)+torch.sum(V*output[:,:-1,1:],dim=1)).item())\n",
    "    print( \"Payoff - p_0 moyen\", torch.mean(-torch.max(test_S1[:,-1,:]-K,torch.zeros(test_S1.shape[0], 1))+test_p_0[:,0,:]).item(), '\\n')\n",
    "\n",
    "\n",
    "t2=time()\n",
    "\n",
    "print( colored('Finale Validation Loss','red') , colored(loss_b.item(),'red'))\n",
    "print( colored('Best Validation Loss', 'green'), min(LOSS_B))\n",
    "print(colored('Finale Test Loss', 'red'), LOSS_test[-1])\n",
    "print(colored('Best Test Loss', 'green' ), min(LOSS_test) )\n",
    "print('Data Size', S1_train500k.shape)\n",
    "print(\"Training Time (h)\", (t2-t1)/3600 ,'\\n') \n",
    "\n",
    "print( \"Evolution during training\")\n",
    "\n",
    "\n",
    "print( \"Quadratic loss evolution\")\n",
    "plt.plot(LOSS_test,color='green')\n",
    "plt.plot(LOSS_B,color='red')\n",
    "plt.show()\n",
    "\n",
    "print(\"Learning Rates\")\n",
    "plt.plot(training_rates)\n",
    "plt.show()\n",
    "\n",
    "print(\"strat test\")\n",
    "plt.plot(strat)\n",
    "plt.show()\n",
    "print( \"PNL Test\")\n",
    "plt.plot(pnl_evol)\n",
    "plt.show()\n",
    "\n",
    "print(\"mediane Test\")\n",
    "plt.plot(mediane)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(x.detach().numpy(),bins=50)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fRNN_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
